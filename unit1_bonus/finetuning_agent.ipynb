{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bonus_unit 1 内容\n",
    "\n",
    "这一节的内容主要是从零微调一个带有tool的调用\n",
    "\n",
    "在我个人的认知里，抄一遍是没有用的，得自己做一遍才有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先挑选一个模型\n",
    "\n",
    "我们需要一个参数较少，我们可以训的起，此外，需要一个暂时没有工具能力的llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Qwen/Qwen2.5-0.5B\"\n",
    "llm = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "            }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What'\\''s the weather like in Boston today?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nWhat\\'\\'\\'s the weather like in Boston today?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([inputs], return_tensors=\"pt\").to(llm.device)\n",
    "response = llm.generate(**inputs, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, response)\n",
    "]\n",
    "\n",
    "response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Boston\", \"unit\": \"celsius\"}}\\neniable\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\nitant\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mua的  这个问题至今不理解， 这个模型讲道理应该只做了pretrain  不知道为啥能做chat\n",
    "\n",
    "但是不管了，反正也能说明对工具不行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = \"Jofthomas/hermes-function-calling-thinking-V1\"\n",
    "raw_dataset = load_dataset(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 3570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': \"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_stock_price', 'description': 'Get the current stock price of a company', 'parameters': {'type': 'object', 'properties': {'company': {'type': 'string', 'description': 'The name of the company'}}, 'required': ['company']}}}, {'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details about a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}}, 'required': ['title']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\",\n",
       "   'role': 'system'},\n",
       "  {'content': 'Hi, can you tell me the current stock price of Apple?',\n",
       "   'role': 'human'},\n",
       "  {'content': \"<think>Alright, the user asked for the current stock price of Apple. Looking at the tools available, there's a function called get_stock_price which exactly fits this request. I'll need to call this function with the parameter company set to 'Apple' to retrieve the information the user is asking for. This seems like the most straightforward and appropriate action to take.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Apple'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': \"<tool_response>\\n{'stock_price': '$150.75'}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'The current stock price of Apple is $150.75.', 'role': 'model'},\n",
       "  {'content': \"That's great. Can you also tell me the stock price for Microsoft?\",\n",
       "   'role': 'human'},\n",
       "  {'content': \"<think>Alright, let me try to figure out how to handle this user's request. The user first asked about Apple's stock price, and I responded by calling the get_stock_price function with 'Apple' as the argument. The tool provided the price, and I passed that information back to the user.\\n\\nNow, the user is asking for the stock price of Microsoft. I need to determine the appropriate action here. Looking at the tools available, there's still the get_stock_price function, which is designed to fetch current stock prices. Since the user is requesting another company's stock price, it makes sense to use the same function again but with the company name changed to Microsoft. \\n\\nI shouldn't assume any other tools are needed because the available functions are specifically tailored for stock prices and movie details. In this case, the relevant function is get_stock_price. I should ensure that the function is called with the correct parameter, 'Microsoft' in this case, to retrieve the accurate information the user is seeking. This approach is efficient and aligns directly with the user's request without making unnecessary assumptions or using unrelated tools.\\n</think><tool_call>\\n{'name': 'get_stock_price', 'arguments': {'company': 'Microsoft'}}\\n</tool_call>\",\n",
       "   'role': 'model'},\n",
       "  {'content': \"<tool_response>\\n{'stock_price': '$210.22'}\\n</tool_response>\",\n",
       "   'role': 'tool'},\n",
       "  {'content': 'The current stock price of Microsoft is $210.22.',\n",
       "   'role': 'model'},\n",
       "  {'content': 'Thank you for the information.', 'role': 'human'},\n",
       "  {'content': \"You're welcome! If you have any other questions, feel free to ask.\",\n",
       "   'role': 'model'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13708, 766, 29]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<think>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要增加一些token给我们的模型\n",
    "from enum import Enum\n",
    "\n",
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call=\"<tool_call>\"\n",
    "    eotool_call=\"</tool_call>\"\n",
    "    tool_response=\"<tool_reponse>\"\n",
    "    eotool_response=\"</tool_reponse>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "    \n",
    "# 这里直接复制了教程里提供的Token 事实上这里有些token是不需要的，对于我们的Qwen来说 但是图省事\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint,\n",
    "    additional_special_tokens=ChatmlSpecialTokens.list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里数据查看，更说明了一点，他这个数据里带think的，而我们模型不带，说明可以训"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为完整数据是3570 按3500:70分割\n",
    "raw_dataset = raw_dataset[\"train\"].train_test_split(test_size=70, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 70\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 3500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 70\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_function(examples):\n",
    "    # 这里跟教程不一样 因为我们不使用trl包  而是使用Trainer 需要是token_id的形式\n",
    "    text_list = tokenizer.apply_chat_template(examples[\"conversations\"], add_generation_prompt=True, tokenize=False)\n",
    "    return tokenizer(text_list)\n",
    "\n",
    "\n",
    "dataset = raw_dataset.map(process_function, batched=True, remove_columns=[\"conversations\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [151644,\n",
       "  8948,\n",
       "  198,\n",
       "  2610,\n",
       "  525,\n",
       "  264,\n",
       "  729,\n",
       "  8098,\n",
       "  15235,\n",
       "  1614,\n",
       "  13,\n",
       "  1446,\n",
       "  525,\n",
       "  3897,\n",
       "  448,\n",
       "  729,\n",
       "  32628,\n",
       "  2878,\n",
       "  220,\n",
       "  151665,\n",
       "  151666,\n",
       "  11874,\n",
       "  9492,\n",
       "  38437,\n",
       "  1231,\n",
       "  1618,\n",
       "  825,\n",
       "  476,\n",
       "  803,\n",
       "  5746,\n",
       "  311,\n",
       "  7789,\n",
       "  448,\n",
       "  279,\n",
       "  1196,\n",
       "  3239,\n",
       "  13,\n",
       "  4320,\n",
       "  944,\n",
       "  1281,\n",
       "  31846,\n",
       "  911,\n",
       "  1128,\n",
       "  2750,\n",
       "  311,\n",
       "  19633,\n",
       "  1119,\n",
       "  5746,\n",
       "  90073,\n",
       "  525,\n",
       "  279,\n",
       "  2500,\n",
       "  7375,\n",
       "  25,\n",
       "  151665,\n",
       "  61108,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  1688,\n",
       "  516,\n",
       "  364,\n",
       "  1688,\n",
       "  1210,\n",
       "  5360,\n",
       "  606,\n",
       "  1210,\n",
       "  364,\n",
       "  1836,\n",
       "  32231,\n",
       "  33984,\n",
       "  516,\n",
       "  364,\n",
       "  4684,\n",
       "  1210,\n",
       "  364,\n",
       "  5890,\n",
       "  369,\n",
       "  15556,\n",
       "  3118,\n",
       "  389,\n",
       "  3728,\n",
       "  323,\n",
       "  35005,\n",
       "  516,\n",
       "  364,\n",
       "  13786,\n",
       "  1210,\n",
       "  5360,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  1700,\n",
       "  516,\n",
       "  364,\n",
       "  13193,\n",
       "  1210,\n",
       "  5360,\n",
       "  2527,\n",
       "  1210,\n",
       "  5360,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  917,\n",
       "  516,\n",
       "  364,\n",
       "  4684,\n",
       "  1210,\n",
       "  364,\n",
       "  785,\n",
       "  3728,\n",
       "  311,\n",
       "  2711,\n",
       "  369,\n",
       "  15556,\n",
       "  24731,\n",
       "  364,\n",
       "  66,\n",
       "  55140,\n",
       "  1210,\n",
       "  5360,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  917,\n",
       "  516,\n",
       "  364,\n",
       "  4684,\n",
       "  1210,\n",
       "  364,\n",
       "  785,\n",
       "  35005,\n",
       "  21933,\n",
       "  8275,\n",
       "  2137,\n",
       "  364,\n",
       "  6279,\n",
       "  1210,\n",
       "  2509,\n",
       "  2527,\n",
       "  516,\n",
       "  364,\n",
       "  66,\n",
       "  55140,\n",
       "  660,\n",
       "  3417,\n",
       "  2137,\n",
       "  5360,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  1688,\n",
       "  516,\n",
       "  364,\n",
       "  1688,\n",
       "  1210,\n",
       "  5360,\n",
       "  606,\n",
       "  1210,\n",
       "  364,\n",
       "  35597,\n",
       "  19464,\n",
       "  516,\n",
       "  364,\n",
       "  4684,\n",
       "  1210,\n",
       "  364,\n",
       "  47866,\n",
       "  279,\n",
       "  6010,\n",
       "  1948,\n",
       "  1378,\n",
       "  10468,\n",
       "  516,\n",
       "  364,\n",
       "  13786,\n",
       "  1210,\n",
       "  5360,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  1700,\n",
       "  516,\n",
       "  364,\n",
       "  13193,\n",
       "  1210,\n",
       "  5360,\n",
       "  8611,\n",
       "  1210,\n",
       "  5360,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  917,\n",
       "  516,\n",
       "  364,\n",
       "  4684,\n",
       "  1210,\n",
       "  364,\n",
       "  785,\n",
       "  6238,\n",
       "  3728,\n",
       "  24731,\n",
       "  364,\n",
       "  17997,\n",
       "  1210,\n",
       "  5360,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  917,\n",
       "  516,\n",
       "  364,\n",
       "  4684,\n",
       "  1210,\n",
       "  364,\n",
       "  785,\n",
       "  9106,\n",
       "  3728,\n",
       "  8275,\n",
       "  2137,\n",
       "  364,\n",
       "  6279,\n",
       "  1210,\n",
       "  2509,\n",
       "  8611,\n",
       "  516,\n",
       "  364,\n",
       "  17997,\n",
       "  660,\n",
       "  3417,\n",
       "  25439,\n",
       "  220,\n",
       "  151666,\n",
       "  10253,\n",
       "  279,\n",
       "  2701,\n",
       "  4510,\n",
       "  67,\n",
       "  8159,\n",
       "  1614,\n",
       "  2951,\n",
       "  10802,\n",
       "  369,\n",
       "  1817,\n",
       "  5392,\n",
       "  1618,\n",
       "  498,\n",
       "  686,\n",
       "  1281,\n",
       "  25,\n",
       "  5360,\n",
       "  2102,\n",
       "  1210,\n",
       "  364,\n",
       "  5152,\n",
       "  7220,\n",
       "  516,\n",
       "  364,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  1700,\n",
       "  516,\n",
       "  364,\n",
       "  13193,\n",
       "  1210,\n",
       "  5360,\n",
       "  16370,\n",
       "  1210,\n",
       "  5360,\n",
       "  2102,\n",
       "  1210,\n",
       "  364,\n",
       "  19139,\n",
       "  516,\n",
       "  364,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  1700,\n",
       "  24731,\n",
       "  364,\n",
       "  606,\n",
       "  1210,\n",
       "  5360,\n",
       "  2102,\n",
       "  1210,\n",
       "  364,\n",
       "  675,\n",
       "  516,\n",
       "  364,\n",
       "  1313,\n",
       "  1210,\n",
       "  364,\n",
       "  917,\n",
       "  8275,\n",
       "  2137,\n",
       "  364,\n",
       "  6279,\n",
       "  1210,\n",
       "  2509,\n",
       "  16370,\n",
       "  516,\n",
       "  364,\n",
       "  606,\n",
       "  32741,\n",
       "  2461,\n",
       "  1817,\n",
       "  729,\n",
       "  1618,\n",
       "  470,\n",
       "  264,\n",
       "  2951,\n",
       "  1633,\n",
       "  448,\n",
       "  729,\n",
       "  829,\n",
       "  323,\n",
       "  5977,\n",
       "  2878,\n",
       "  220,\n",
       "  151657,\n",
       "  151658,\n",
       "  11874,\n",
       "  9492,\n",
       "  438,\n",
       "  11017,\n",
       "  510,\n",
       "  151657,\n",
       "  198,\n",
       "  90,\n",
       "  14172,\n",
       "  13429,\n",
       "  532,\n",
       "  151658,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  872,\n",
       "  198,\n",
       "  27,\n",
       "  14172,\n",
       "  9655,\n",
       "  397,\n",
       "  27,\n",
       "  14172,\n",
       "  9655,\n",
       "  397,\n",
       "  13608,\n",
       "  95137,\n",
       "  1210,\n",
       "  61108,\n",
       "  606,\n",
       "  1210,\n",
       "  364,\n",
       "  8852,\n",
       "  19756,\n",
       "  516,\n",
       "  364,\n",
       "  4995,\n",
       "  1210,\n",
       "  364,\n",
       "  16,\n",
       "  23,\n",
       "  16,\n",
       "  25164,\n",
       "  794,\n",
       "  11,\n",
       "  1532,\n",
       "  4261,\n",
       "  11,\n",
       "  12271,\n",
       "  220,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  516,\n",
       "  364,\n",
       "  21931,\n",
       "  1210,\n",
       "  220,\n",
       "  19,\n",
       "  13,\n",
       "  22,\n",
       "  2137,\n",
       "  5360,\n",
       "  606,\n",
       "  1210,\n",
       "  330,\n",
       "  43,\n",
       "  6,\n",
       "  9286,\n",
       "  52813,\n",
       "  497,\n",
       "  364,\n",
       "  4995,\n",
       "  1210,\n",
       "  364,\n",
       "  17,\n",
       "  17,\n",
       "  23,\n",
       "  467,\n",
       "  220,\n",
       "  16,\n",
       "  15,\n",
       "  339,\n",
       "  794,\n",
       "  11,\n",
       "  1532,\n",
       "  4261,\n",
       "  11,\n",
       "  12271,\n",
       "  220,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  19,\n",
       "  516,\n",
       "  364,\n",
       "  21931,\n",
       "  1210,\n",
       "  220,\n",
       "  19,\n",
       "  13,\n",
       "  21,\n",
       "  2137,\n",
       "  5360,\n",
       "  606,\n",
       "  1210,\n",
       "  364,\n",
       "  12050,\n",
       "  29351,\n",
       "  3313,\n",
       "  516,\n",
       "  364,\n",
       "  4995,\n",
       "  1210,\n",
       "  364,\n",
       "  23,\n",
       "  21,\n",
       "  467,\n",
       "  220,\n",
       "  18,\n",
       "  6498,\n",
       "  794,\n",
       "  11,\n",
       "  1532,\n",
       "  4261,\n",
       "  11,\n",
       "  12271,\n",
       "  220,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  516,\n",
       "  364,\n",
       "  21931,\n",
       "  1210,\n",
       "  220,\n",
       "  19,\n",
       "  13,\n",
       "  19,\n",
       "  92,\n",
       "  23439,\n",
       "  522,\n",
       "  14172,\n",
       "  9655,\n",
       "  397,\n",
       "  522,\n",
       "  14172,\n",
       "  9655,\n",
       "  29,\n",
       "  151645,\n",
       "  198,\n",
       "  151644,\n",
       "  77091,\n",
       "  198],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(151673, 896)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.resize_token_embeddings(len(tokenizer))  # 会修改model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "llm_lora = get_peft_model(llm, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,337,792 || trainable%: 0.1094\n"
     ]
    }
   ],
   "source": [
    "llm_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    report_to=\"wandb\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=5,\n",
    "    bf16=True,\n",
    "    gradient_accumulation_steps=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    do_eval=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    llm_lora, \n",
    "    args, \n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: yizhen-ciao (yizhen-ciao-student) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\code\\huggingface_agent_course\\unit1_bonus\\wandb\\run-20250309_211204-jxt8f1up</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yizhen-ciao-student/huggingface/runs/jxt8f1up' target=\"_blank\">output</a></strong> to <a href='https://wandb.ai/yizhen-ciao-student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yizhen-ciao-student/huggingface' target=\"_blank\">https://wandb.ai/yizhen-ciao-student/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yizhen-ciao-student/huggingface/runs/jxt8f1up' target=\"_blank\">https://wandb.ai/yizhen-ciao-student/huggingface/runs/jxt8f1up</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [525/525 14:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.660400</td>\n",
       "      <td>0.661976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.528925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.513666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\anaconda\\envs\\huggingface_agent_course\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "d:\\software\\anaconda\\envs\\huggingface_agent_course\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "d:\\software\\anaconda\\envs\\huggingface_agent_course\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "d:\\software\\anaconda\\envs\\huggingface_agent_course\\Lib\\site-packages\\peft\\utils\\save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=525, training_loss=0.832591903323219, metrics={'train_runtime': 879.1027, 'train_samples_per_second': 11.944, 'train_steps_per_second': 0.597, 'total_flos': 1.1381457960677376e+16, 'train_loss': 0.832591903323219, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训完测效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraModel, PeftConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Qwen/Qwen2.5-0.5B\"\n",
    "llm = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要增加一些token给我们的模型\n",
    "from enum import Enum\n",
    "\n",
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    tools = \"<tools>\"\n",
    "    eotools = \"</tools>\"\n",
    "    think = \"<think>\"\n",
    "    eothink = \"</think>\"\n",
    "    tool_call=\"<tool_call>\"\n",
    "    eotool_call=\"</tool_call>\"\n",
    "    tool_response=\"<tool_reponse>\"\n",
    "    eotool_response=\"</tool_reponse>\"\n",
    "    pad_token = \"<pad>\"\n",
    "    eos_token = \"<eos>\"\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "    \n",
    "# 这里直接复制了教程里提供的Token 事实上这里有些token是不需要的，对于我们的Qwen来说 但是图省事\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint,\n",
    "    additional_special_tokens=ChatmlSpecialTokens.list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7004\\2513183782.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  lora_config = PeftConfig.from_pretrained(\"output\\checkpoint-525\")\n"
     ]
    }
   ],
   "source": [
    "lora_config = PeftConfig.from_pretrained(\"output\\checkpoint-525\")\n",
    "llm_lora = LoraModel(llm, config=lora_config, adapter_name=\"tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "            }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What'\\''s the weather like in Boston today?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather in a given location\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}}, \"required\": [\"location\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nWhat\\'\\'\\'s the weather like in Boston today?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([inputs], return_tensors=\"pt\").to(llm_lora.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,    382,\n",
       "              2,  13852,    271,   2610,   1231,   1618,    825,    476,    803,\n",
       "           5746,    311,   7789,    448,    279,   1196,   3239,    382,   2610,\n",
       "            525,   3897,    448,    729,  32628,   2878,    220, 151665, 151666,\n",
       "          11874,   9492,    510, 151665,    198,   4913,   1313,    788,    330,\n",
       "           1688,    497,    330,   1688,    788,   5212,    606,    788,    330,\n",
       "            455,  11080,  69364,    497,    330,   4684,    788,    330,   1949,\n",
       "            279,   1482,   9104,    304,    264,   2661,   3728,    497,    330,\n",
       "          13786,    788,   5212,   1313,    788,    330,   1700,    497,    330,\n",
       "          13193,    788,   5212,   2527,    788,   5212,   1313,    788,    330,\n",
       "            917,    497,    330,   4684,    788,    330,    785,   3283,    323,\n",
       "           1584,     11,    384,   1302,     13,   5836,  12879,     11,   9183,\n",
       "          14345,    330,   3843,    788,   5212,   1313,    788,    330,    917,\n",
       "            497,    330,   9018,    788,   4383,     66,  40247,    497,    330,\n",
       "             69,  47910,   1341,  38154,    330,   6279,    788,   4383,   2527,\n",
       "           1341,   3417,    532, 151666,    271,   2461,   1817,    729,   1618,\n",
       "             11,    470,    264,   2951,   1633,    448,    729,    829,    323,\n",
       "           5977,   2878,    220, 151657, 151658,  11874,   9492,    510, 151657,\n",
       "            198,   4913,    606,    788,    366,   1688,  11494,   8066,    330,\n",
       "          16370,    788,    366,   2116,  56080,  40432,  31296, 151658, 151645,\n",
       "            198, 151644,    872,    198,   3838,  18788,     82,    279,   9104,\n",
       "           1075,    304,  10196,   3351,     30, 151645,    198, 151644,  77091,\n",
       "            198,   4913,    606,    788,    330,    455,  11080,  69364,    497,\n",
       "            330,  16370,    788,   5212,   2527,    788,    330,  64332,    497,\n",
       "            330,   3843,    788,    330,     66,  40247,  95642,  75049,    198,\n",
       "           3838,  18788,     82,    279,   9104,   1075,    304,  10196,   3351,\n",
       "             30,  75049,    198,  75049,    198,   3838,  18788,     82,    279,\n",
       "           9104,   1075,    304,  10196,   3351,     30,  75049,    198,  75049,\n",
       "            198,   3838,  18788,     82,    279,   9104,   1075,    304,  10196,\n",
       "           3351,     30,  75049,    198,  75049,    198,   3838,  18788,     82,\n",
       "            279,   9104,   1075,    304,  10196,   3351,     30,  75049,    198,\n",
       "          75049,    198,   3838,  18788,     82,    279,   9104,   1075,    304,\n",
       "          10196,   3351,     30,  75049,    198,  75049,    198,   3838,  18788,\n",
       "             82,    279,   9104,   1075,    304,  10196,   3351,     30,  75049,\n",
       "            198,  75049,    198,   3838,  18788,     82,    279,   9104,   1075,\n",
       "            304,  10196,   3351,     30,  75049,    198,  75049,    198,   3838,\n",
       "          18788,     82,    279,   9104,   1075,    304,  10196,   3351,     30,\n",
       "          75049,    198,  75049,    198,   3838,  18788,     82,    279,   9104,\n",
       "           1075,    304,  10196,   3351,     30,  75049,    198,  75049,    198,\n",
       "           3838,  18788,     82,    279,   9104,   1075,    304,  10196,   3351,\n",
       "             30,  75049,    198,  75049,    198,   3838,  18788,     82,    279,\n",
       "           9104,   1075,    304,  10196,   3351,     30,  75049,    198,  75049,\n",
       "            198,   3838,  18788,     82,    279,   9104,   1075,    304,  10196,\n",
       "           3351,     30,  75049,    198,  75049,    198,   3838,  18788,     82,\n",
       "            279,   9104,   1075,    304,  10196,   3351,     30,  75049,    198,\n",
       "          75049,    198,   3838,  18788,     82,    279,   9104,   1075,    304,\n",
       "          10196,   3351,     30,  75049,    198,  75049,    198,   3838,  18788,\n",
       "             82,    279,   9104,   1075,    304,  10196,   3351,     30,  75049,\n",
       "            198,  75049,    198,   3838,  18788,     82,    279,   9104,   1075,\n",
       "            304,  10196,   3351,     30,  75049,    198,  75049,    198,   3838,\n",
       "          18788,     82,    279,   9104,   1075,    304,  10196,   3351,     30,\n",
       "          75049,    198,  75049,    198,   3838,  18788,     82,    279,   9104,\n",
       "           1075,    304,  10196,   3351,     30,  75049,    198,  75049,    198,\n",
       "           3838,  18788,     82,    279,   9104,   1075,    304,  10196,   3351,\n",
       "             30,  75049,    198,  75049,    198,   3838,  18788,     82,    279,\n",
       "           9104,   1075,    304,  10196,   3351,     30,  75049,    198,  75049,\n",
       "            198,   3838,  18788,     82,    279,   9104,   1075,    304,  10196,\n",
       "           3351,     30,  75049,    198,  75049,    198,   3838,  18788,     82,\n",
       "            279,   9104,   1075,    304,  10196,   3351,     30,  75049,    198,\n",
       "          75049,    198,   3838,  18788,     82,    279,   9104,   1075,    304,\n",
       "          10196,   3351,     30,  75049,    198,  75049,    198,   3838,  18788,\n",
       "             82,    279,   9104,   1075,    304,  10196,   3351,     30,  75049,\n",
       "            198,  75049,    198,   3838,  18788,     82,    279,   9104,   1075,\n",
       "            304,  10196,   3351,     30,  75049,    198,  75049,    198,   3838,\n",
       "          18788,     82,    279,   9104,   1075,    304,  10196,   3351,     30,\n",
       "          75049,    198,  75049,    198,   3838,  18788,     82,    279,   9104,\n",
       "           1075,    304,  10196,   3351,     30,  75049,    198,  75049,    198,\n",
       "           3838,  18788,     82,    279,   9104,   1075,    304,  10196,   3351,\n",
       "             30,  75049,    198,  75049,    198,   3838,  18788,     82,    279,\n",
       "           9104,   1075,    304,  10196,   3351,     30,  75049,    198,  75049,\n",
       "            198,   3838,  18788,     82,    279,   9104,   1075,    304,  10196,\n",
       "           3351,     30,  75049,    198,  75049,    198,   3838,  18788,     82,\n",
       "            279,   9104,   1075,    304,  10196,   3351,     30,  75049,    198,\n",
       "          75049,    198,   3838,  18788,     82,    279,   9104,   1075,    304,\n",
       "          10196,   3351,     30,  75049,    198,  75049,    198,   3838,  18788,\n",
       "             82,    279,   9104,   1075,    304,  10196,   3351,     30,  75049,\n",
       "            198,  75049,    198,   3838,  18788,     82,    279,   9104,   1075,\n",
       "            304,  10196,   3351,     30,  75049,    198,  75049,    198,   3838,\n",
       "          18788,     82,    279,   9104,   1075,    304,  10196,   3351,     30]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_lora.generate(**inputs, max_new_tokens=512)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, response)\n",
    "]\n",
    "\n",
    "response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Boston\", \"unit\": \"celsius\"}}\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?ponde\n",
      "ponde\n",
      "What'''s the weather like in Boston today?\n"
     ]
    }
   ],
   "source": [
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 这个要重新训， 有问题\n",
    "\n",
    "应该是需要训练数据需要重新解析一下，tools 以及结束符号有问题，待做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_agent_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
